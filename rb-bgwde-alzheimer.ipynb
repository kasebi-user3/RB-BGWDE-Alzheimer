{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, label_binarize\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.feature_selection import VarianceThreshold, RFE\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve, auc\nfrom sklearn.ensemble import StackingClassifier, RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom boruta import BorutaPy\nimport random\nimport matplotlib.pyplot as plt\nimport warnings\nfrom scipy import stats\n\nwarnings.filterwarnings(\"ignore\")\n\n# Improved fitness function combining accuracy and AUC-ROC\n\nclass XGBFitness:\n    def __init__(self, X, y, cv=10, **xgb_params):\n        self.X = X\n        self.y = y\n        self.cv = cv\n        self.xgb_params = xgb_params\n\n    def __call__(self, binary_mask):\n        if np.count_nonzero(binary_mask) == 0:\n            return 1.0\n        X_selected = self.X[:, binary_mask == 1]\n        clf = XGBClassifier(\n            eval_metric='logloss',\n            verbosity=0,\n            use_label_encoder=False,\n            **self.xgb_params\n        )\n        accuracy_scores = cross_val_score(clf, X_selected, self.y, cv=self.cv, scoring='accuracy', n_jobs=-1)\n        auc_scores = cross_val_score(clf, X_selected, self.y, cv=self.cv, scoring='roc_auc', n_jobs=-1)\n        return 1 - (0.7 * np.mean(accuracy_scores) + 0.3 * np.mean(auc_scores))\n\n\n# Binary Grey Wolf Optimizer with Differential Evolution (RBGWO-DE)\n\nclass RBGWO_DE:\n    def __init__(self, obj_func, trans_func, problem_size=50, domain_range=(-1, 1),\n                 log=True, epoch=100, pop_size=10, lsa_epoch=20):\n        self.obj_func = obj_func\n        self.trans_func = trans_func\n        self.problem_size = problem_size\n        self.domain_range = domain_range\n        self.log = log\n        self.epoch = epoch\n        self.pop_size = pop_size\n        self.lsa_epoch = lsa_epoch\n        self.solution = None\n        self.best_fitness = float(\"inf\")\n\n    def _binary_conversion(self, pos):\n        transfer_val = abs(np.tanh(pos))  # v-shape transfer\n        return np.where(np.random.rand(*pos.shape) < transfer_val, 1, 0)\n\n    def _de_local_search(self, position):\n        best_pos = position.copy()\n        best_fit = self.obj_func(self._binary_conversion(best_pos))\n        for _ in range(self.lsa_epoch):\n            scale = np.random.uniform(0.2, 0.8)\n            mutant = best_pos + scale * (np.random.uniform(-1, 1, self.problem_size))\n            mutant = np.clip(mutant, self.domain_range[0], self.domain_range[1])\n            bin_mutant = self._binary_conversion(mutant)\n            fit = self.obj_func(bin_mutant)\n            if fit < best_fit:\n                best_pos = mutant\n                best_fit = fit\n        return best_pos, best_fit\n\n    def train(self):\n        print(\"Starting RBGWO-DE optimization...\")\n        pop = np.random.uniform(self.domain_range[0], self.domain_range[1],\n                                (self.pop_size, self.problem_size))\n        bin_pop = np.array([self._binary_conversion(ind) for ind in pop])\n        fitness = np.array([self.obj_func(ind) for ind in bin_pop])\n        alpha, beta, delta = [np.zeros(self.problem_size)] * 3\n        alpha_score, beta_score, delta_score = [float(\"inf\")] * 3\n\n        for epoch in range(self.epoch):\n            a = 2 - epoch * (2 / self.epoch)\n            for i in range(self.pop_size):\n                if fitness[i] < alpha_score:\n                    delta, delta_score = beta, beta_score\n                    beta, beta_score = alpha, alpha_score\n                    alpha, alpha_score = pop[i].copy(), fitness[i]\n                elif fitness[i] < beta_score:\n                    delta, delta_score = beta, beta_score\n                    beta, beta_score = pop[i].copy(), fitness[i]\n                elif fitness[i] < delta_score:\n                    delta, delta_score = pop[i].copy(), fitness[i]\n\n            for i in range(self.pop_size):\n                r1, r2 = random.random(), random.random()\n                A1, C1 = 2 * a * r1 - a, 2 * r2\n                D_alpha = abs(C1 * alpha - pop[i])\n                X1 = alpha - A1 * D_alpha\n                r1, r2 = random.random(), random.random()\n                A2, C2 = 2 * a * r1 - a, 2 * r2\n                D_beta = abs(C2 * beta - pop[i])\n                X2 = beta - A2 * D_beta\n                r1, r2 = random.random(), random.random()\n                A3, C3 = 2 * a * r1 - a, 2 * r2\n                D_delta = abs(C3 * delta - pop[i])\n                X3 = delta - A3 * D_delta\n                new_pos = (X1 + X2 + X3) / 3.0\n                new_pos = np.clip(new_pos, self.domain_range[0], self.domain_range[1])\n                new_pos, new_fit = self._de_local_search(new_pos)\n                pop[i] = new_pos\n                fitness[i] = new_fit\n\n            best_idx = np.argmin(fitness)\n            self.solution = self._binary_conversion(pop[best_idx])\n            self.best_fitness = fitness[best_idx]\n            if self.log:\n                print(f\"Epoch {epoch+1}/{self.epoch}, Best Fitness: {self.best_fitness}\")\n\n        print(\"RBGWO-DE optimization finished.\")\n        return self.solution, self.best_fitness\n\n\n# Main processing\n\nprint(\"1. Loading dataset...\")\ntry:\n    data = pd.read_csv(\"/kaggle/input/alzheimers-disease-dataset/alzheimers_disease_data.csv\", encoding='utf-8', sep=',')\nexcept Exception as e:\n    print(f\"Error reading CSV file: {e}\")\n    raise\n\nprint(\"Dataset columns:\", data.columns.tolist())\n\nprint(\"2. Dropping unnecessary columns...\")\nfor col_to_drop in ['DoctorInCharge', 'PatientID']:\n    if col_to_drop in data.columns:\n        print(f\"Dropping '{col_to_drop}'\")\n        data = data.drop(columns=[col_to_drop])\n\nprint(\"3. Checking class distribution...\")\nprint(data['Diagnosis'].value_counts())\n\n# Define numerical and categorical columns\nnumerical_cols = [\n    'Age', 'BMI', 'AlcoholConsumption', 'PhysicalActivity', 'DietQuality',\n    'SleepQuality', 'SystolicBP', 'DiastolicBP', 'CholesterolTotal',\n    'CholesterolLDL', 'CholesterolHDL', 'CholesterolTriglycerides',\n    'MMSE', 'FunctionalAssessment', 'ADL'\n]\ncategorical_cols = [\n    'Gender', 'Ethnicity', 'EducationLevel', 'Smoking', 'FamilyHistoryAlzheimers',\n    'CardiovascularDisease', 'Diabetes', 'Depression', 'HeadInjury', 'Hypertension',\n    'MemoryComplaints', 'BehavioralProblems', 'Confusion', 'Disorientation',\n    'PersonalityChanges', 'DifficultyCompletingTasks', 'Forgetfulness'\n]\n\n# Verify columns\nmissing_numerical = [col for col in numerical_cols if col not in data.columns]\nmissing_categorical = [col for col in categorical_cols if col not in data.columns]\nif missing_numerical or missing_categorical:\n    print(f\"Missing numerical columns: {missing_numerical}\")\n    print(f\"Missing categorical columns: {missing_categorical}\")\n    raise SystemExit(\"Dataset doesn't contain expected columns.\")\n\nprint(\"4. Handling missing values...\")\nnum_imputer = SimpleImputer(strategy='mean')\ncat_imputer = SimpleImputer(strategy='most_frequent')\ndata[numerical_cols] = num_imputer.fit_transform(data[numerical_cols])\ndata[categorical_cols] = cat_imputer.fit_transform(data[categorical_cols])\n\n# Check for outliers\nprint(\"5. Checking for outliers...\")\nz_scores = stats.zscore(data[numerical_cols])\ndata = data[(z_scores < 3.5).all(axis=1)]\nprint(f\"Dataset size after outlier removal: {data.shape}\")\n\nprint(\"6. Separating features and target...\")\nX = data.drop(columns=['Diagnosis'])\ny = data['Diagnosis']\n\nprint(\"7. Preprocessing pipeline...\")\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', Pipeline([\n            ('imputer', SimpleImputer(strategy='mean')),\n            ('scaler', StandardScaler()),\n            ('variance', VarianceThreshold(threshold=0.01))\n        ]), numerical_cols),\n        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_cols)\n    ])\nX_processed = preprocessor.fit_transform(X)\n\nprint(\"8. Splitting data into train and test sets...\")\nX_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42, stratify=y)\n\nprint(\"9. Balancing training data with SMOTE...\")\nsmote = SMOTE(random_state=42)\nX_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n\nprint(\"10. BorutaPy feature selection...\")\nrf_boruta = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\nboruta_selector = BorutaPy(rf_boruta, n_estimators='auto', perc=70, verbose=2, random_state=42, max_iter=30)\nboruta_selector.fit(X_train_balanced, y_train_balanced)\n\nboruta_mask = boruta_selector.support_ | boruta_selector.support_weak_\nX_boruta = X_train_balanced[:, boruta_mask]\nX_test_boruta = X_test[:, boruta_mask]\nselected_features = np.array(preprocessor.get_feature_names_out())[boruta_mask]\nprint(f\"Boruta selected {X_boruta.shape[1]} features (including tentative):\")\nprint(selected_features)\n\nprint(\"11. Defining fitness function for XGBoost...\")\nxgb_params = {\n    'n_estimators': 100,\n    'max_depth': 5,\n    'learning_rate': 0.1,\n    'reg_lambda': 1.0,\n    'gamma': 0.1\n}\nfitness_function = XGBFitness(X_boruta, y_train_balanced, cv=10, **xgb_params)  # cv reduced for speed\n\nprint(\"12. Initializing and running RBGWO-DE optimizer...\")\noptimizer = RBGWO_DE(\n    obj_func=fitness_function,\n    trans_func=lambda x: abs(np.tanh(x)),\n    problem_size=X_boruta.shape[1],\n    domain_range=(-10, 10),\n    epoch=5,        # keep small for testing; increase for production\n    pop_size=20,\n    lsa_epoch=10,\n    log=True\n)\nbest_solution, best_fitness = optimizer.train()\n\nprint(\"13. Applying RFE on selected features...\")\nX_train_selected = X_boruta[:, best_solution == 1]\nn_features_rfe = min(10, X_train_selected.shape[1])\nif n_features_rfe < 1:\n    raise SystemExit(\"No features selected by optimizer/Boruta/RFE pipeline.\")\nxgb_base = XGBClassifier(**xgb_params)\nrfe = RFE(estimator=xgb_base, n_features_to_select=n_features_rfe)\nX_train_rfe = rfe.fit_transform(X_train_selected, y_train_balanced)\nrfe_mask = rfe.support_\nX_test_rfe = X_test_boruta[:, best_solution == 1][:, rfe_mask]\nfinal_features = selected_features[best_solution == 1][rfe_mask]\nprint(f\"RFE selected {X_train_rfe.shape[1]} features:\")\nprint(final_features)\n\n\n# 14. Hyperparameter tuning for XGBoost\n\nxgb_param_dist = {\n    'n_estimators': [50, 100, 200, 300, 500],\n    'max_depth': [3, 5, 7, 9, 12],\n    'learning_rate': [0.01, 0.05, 0.1, 0.3, 0.5],\n    'reg_lambda': [0.01, 0.1, 1.0, 10.0, 100.0],\n    'gamma': [0, 0.1, 0.5, 1.0],\n    'subsample': [0.5, 0.6, 0.8, 1.0],\n    'colsample_bytree': [0.5, 0.6, 0.8, 1.0]\n}\nxgb = XGBClassifier(eval_metric='logloss', verbosity=0, use_label_encoder=False, random_state=42)\nxgb_search = RandomizedSearchCV(xgb, xgb_param_dist, n_iter=50, cv=10, scoring='accuracy', n_jobs=-1, random_state=42)\nxgb_search.fit(X_train_rfe, y_train_balanced)\nbest_xgb = xgb_search.best_estimator_\nprint(\"Best XGB params:\", xgb_search.best_params_)\n\n\n# 15. Hyperparameter tuning for RandomForest\n\nrf_param_dist = {\n    'n_estimators': [50, 100, 200, 300],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\nrf = RandomForestClassifier(random_state=42)\nrf_search = RandomizedSearchCV(rf, rf_param_dist, n_iter=50, cv=10, scoring='accuracy', n_jobs=-1, random_state=42)\nrf_search.fit(X_train_rfe, y_train_balanced)\nbest_rf = rf_search.best_estimator_\nprint(\"Best RF params:\", rf_search.best_params_)\n\n\n# 16. Hyperparameter tuning for SVM\n\nsvm_param_dist = {\n    'C': [0.1, 1, 10, 100],\n    'kernel': ['rbf', 'linear'],\n    'gamma': ['scale', 'auto', 0.01, 0.1, 1.0]\n}\nsvm = SVC(probability=True, random_state=42)\nsvm_search = RandomizedSearchCV(svm, svm_param_dist, n_iter=50, cv=10, scoring='accuracy', n_jobs=-1, random_state=42)\nsvm_search.fit(X_train_rfe, y_train_balanced)\nbest_svm = svm_search.best_estimator_\nprint(\"Best SVM params:\", svm_search.best_params_)\n\n\n# 17. Hyperparameter tuning for LightGBM\n\nlgbm_param_dist = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [3, 5, 7],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'reg_lambda': [0.1, 1.0, 10.0],\n    'subsample': [0.6, 0.8, 1.0],\n    'colsample_bytree': [0.6, 0.8, 1.0]\n}\nlgbm = LGBMClassifier(random_state=42)\nlgbm_search = RandomizedSearchCV(lgbm, lgbm_param_dist, n_iter=50, cv=10, scoring='accuracy', n_jobs=-1, random_state=42)\nlgbm_search.fit(X_train_rfe, y_train_balanced)\nbest_lgbm = lgbm_search.best_estimator_\nprint(\"Best LGBM params:\", lgbm_search.best_params_)\n\n\n# 18. Training StackingClassifier ensemble\n\nstacking_clf = StackingClassifier(\n    estimators=[\n        ('xgb', best_xgb),\n        ('rf', best_rf),\n        ('svm', best_svm),\n        ('lgbm', best_lgbm)\n    ],\n    final_estimator=XGBClassifier(eval_metric='logloss', use_label_encoder=False, random_state=42),\n    cv=10,\n    n_jobs=-1,\n    passthrough=False\n)\nstacking_clf.fit(X_train_rfe, y_train_balanced)\n\n\n# 19. Evaluating ensemble model\n\n# Train set evaluation\ny_train_pred = stacking_clf.predict(X_train_rfe)\ntrain_accuracy = accuracy_score(y_train_balanced, y_train_pred)\nprint(\"Train Accuracy:\", train_accuracy)\n\n# Test set evaluation\ny_test_pred = stacking_clf.predict(X_test_re := X_test_rfe)  # assign for clarity\ntest_accuracy = accuracy_score(y_test, y_test_pred)\nprint(\"Test Accuracy:\", test_accuracy)\nprint(\"Classification Report:\\n\", classification_report(y_test, y_test_pred))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred))\n\n\n# AUC-ROC and Precision-Recall (binary & multiclass safe)\n\nprint(\"Calculating AUC / ROC / Precision-Recall...\")\n# predict_proba (StackingClassifier uses final estimator; base estimators must provide predict_proba)\nif not hasattr(stacking_clf, \"predict_proba\"):\n    raise RuntimeError(\"stacking_clf has no predict_proba; ensure base estimators and final estimator support predict_proba.\")\n\ny_pred_proba = stacking_clf.predict_proba(X_test_rfe)\nprint(\"Shapes -> y_test:\", y_test.shape, \", y_pred_proba:\", y_pred_proba.shape)\n\nclasses = stacking_clf.classes_\nn_classes = y_pred_proba.shape[1]\n\nif n_classes == 2:\n    # binary case: make y_true binary vector for the positive class (classes[1])\n    positive_class = classes[1]\n    y_true_bin = (y_test == positive_class).astype(int)\n    auc_roc = roc_auc_score(y_true_bin, y_pred_proba[:, 1])\n    fpr, tpr, _ = roc_curve(y_true_bin, y_pred_proba[:, 1])\n    precision_vals, recall_vals, _ = precision_recall_curve(y_true_bin, y_pred_proba[:, 1])\n\n    # ROC\n    plt.figure()\n    plt.plot(fpr, tpr, label=f'ROC curve (AUC = {auc_roc:.4f})')\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve (binary)')\n    plt.legend(loc=\"lower right\")\n    plt.savefig('roc_curve.png')\n    plt.close()\n\n    # Precision-Recall\n    plt.figure()\n    plt.plot(recall_vals, precision_vals, label='Precision-Recall curve')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision-Recall Curve (binary)')\n    plt.legend(loc=\"lower left\")\n    plt.savefig('precision_recall_curve.png')\n    plt.close()\n\n    print(f\"Binary AUC-ROC: {auc_roc:.4f}\")\n\nelse:\n    # multiclass case: binarize y_test using same class order as stacking_clf.classes_\n    y_test_bin = label_binarize(y_test, classes=classes)\n    auc_roc_macro = roc_auc_score(y_test_bin, y_pred_proba, multi_class='ovr', average='macro')\n    auc_roc_weighted = roc_auc_score(y_test_bin, y_pred_proba, multi_class='ovr', average='weighted')\n    print(f\"Multi-class AUC-ROC (macro OVR): {auc_roc_macro:.4f}\")\n    print(f\"Multi-class AUC-ROC (weighted OVR): {auc_roc_weighted:.4f}\")\n\n    # Per-class ROC\n    fpr = dict(); tpr = dict(); roc_auc_dict = dict()\n    for i in range(n_classes):\n        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_proba[:, i])\n        roc_auc_dict[i] = auc(fpr[i], tpr[i])\n\n    # micro-average\n    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_bin.ravel(), y_pred_proba.ravel())\n    roc_auc_dict[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n    # Plot ROC\n    plt.figure()\n    plt.plot(fpr[\"micro\"], tpr[\"micro\"], label=f'micro-average ROC (AUC = {roc_auc_dict[\"micro\"]:.4f})', linewidth=2)\n    for i in range(n_classes):\n        plt.plot(fpr[i], tpr[i], label=f'Class {classes[i]} (AUC = {roc_auc_dict[i]:.4f})')\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve (multi-class)')\n    plt.legend(loc=\"lower right\")\n    plt.savefig('roc_curve.png')\n    plt.close()\n\n    # Precision-Recall per class + AUC(PR)\n    precision = dict(); recall = dict(); pr_auc = dict()\n    plt.figure()\n    for i in range(n_classes):\n        precision[i], recall[i], _ = precision_recall_curve(y_test_bin[:, i], y_pred_proba[:, i])\n        pr_auc[i] = auc(recall[i], precision[i])\n        plt.plot(recall[i], precision[i], label=f'Class {classes[i]} (AUC = {pr_auc[i]:.4f})')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision-Recall Curve (multi-class)')\n    plt.legend(loc=\"lower left\")\n    plt.savefig('precision_recall_curve.png')\n    plt.close()\n\nprint(\"All steps completed successfully.\")\n\n# Additional evaluation: confusion matrices, train/val curves (XGB/LGBM), stacking final eval\n\nimport itertools\nfrom sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay, PrecisionRecallDisplay\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport os\n\nif not os.path.isdir(\"eval_plots\"):\n    os.makedirs(\"eval_plots\")\n\ndef plot_confusion(y_true, y_pred, labels, title, filename):\n    cm = confusion_matrix(y_true, y_pred, labels=labels)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n    fig, ax = plt.subplots(figsize=(6,6))\n    disp.plot(ax=ax, cmap=plt.cm.Blues, values_format='d')\n    ax.set_title(title)\n    plt.tight_layout()\n    fig.savefig(filename)\n    plt.close(fig)\n\ndef plot_train_val_curves_from_xgb(clf, X_tr, y_tr, X_val, y_val, label_prefix, filename_prefix):\n    # clf should be an XGBClassifier with n_estimators set\n    eval_set = [(X_tr, y_tr), (X_val, y_val)]\n    # clone params to ensure verbosity off, etc.\n    params = clf.get_xgb_params()\n    # train a fresh xgb to capture evals_result (use same params)\n    import xgboost as xgb\n    dtrain = xgb.DMatrix(X_tr, label=y_tr)\n    dval = xgb.DMatrix(X_val, label=y_val)\n    num_boost_round = clf.get_params().get(\"n_estimators\", 100)\n    xgb_params = params.copy()\n    # ensure objective appropriate for multiclass/binary\n    if len(np.unique(y_tr)) > 2:\n        xgb_params.setdefault(\"objective\", \"multi:softprob\")\n        xgb_params[\"num_class\"] = len(np.unique(y_tr))\n        eval_metric = \"mlogloss\"\n    else:\n        xgb_params.setdefault(\"objective\", \"binary:logistic\")\n        eval_metric = \"logloss\"\n    xgb_params[\"verbosity\"] = 0\n    # use early stopping to find best_iteration\n    bst = xgb.train(xgb_params, dtrain, num_boost_round=num_boost_round,\n                    evals=[(dtrain, \"train\"), (dval, \"validation\")],\n                    early_stopping_rounds=20, evals_result={}, verbose_eval=False)\n    results = bst.eval_history if hasattr(bst, \"eval_history\") else bst.attributes()\n    # safer: get evals_result via bst.eval_set if present\n    ev = bst.eval_result() if hasattr(bst, \"eval_result\") else None\n\n    evals_result = bst.eval_result() if hasattr(bst, \"eval_result\") else getattr(bst, \"evals_result\", None)\n    # xgboost python API versions differ; robustly try to get evals_result\n    try:\n        evals_result = bst.eval_result()\n    except Exception:\n        try:\n            evals_result = bst.evals_result()\n        except Exception:\n            evals_result = getattr(bst, \"evals_result\", None)\n\n    if evals_result is None:\n        # fallback: use sklearn wrapper with learning_monitor\n        print(f\"Could not extract evals_result for {label_prefix}; skipping curve plot.\")\n        return None\n\n    # determine metric name (first key in evals_result['train'])\n    train_metrics = evals_result.get(\"train\", {})\n    val_metrics = evals_result.get(\"validation\", {})\n    metric_name = list(train_metrics.keys())[0]\n    train_vals = train_metrics[metric_name]\n    val_vals = val_metrics[metric_name]\n\n    epochs = list(range(1, len(train_vals)+1))\n\n    # Plot metric (logloss or mlogloss)\n    fig, ax = plt.subplots()\n    ax.plot(epochs, train_vals, label=f\"train {metric_name}\")\n    ax.plot(epochs, val_vals, label=f\"val {metric_name}\")\n    ax.axvline(bst.best_iteration+1 if hasattr(bst, \"best_iteration\") else None, color='grey', linestyle='--',\n               label=f\"best_iteration={getattr(bst, 'best_iteration', 'NA')}\")\n    ax.set_xlabel(\"Boosting round\")\n    ax.set_ylabel(metric_name)\n    ax.set_title(f\"{label_prefix} {metric_name} per boosting round\")\n    ax.legend()\n    plt.tight_layout()\n    fig.savefig(f\"{filename_prefix}_{metric_name}.png\")\n    plt.close(fig)\n    return getattr(bst, \"best_iteration\", None)\n\ndef plot_train_val_curves_from_lgbm(clf, X_tr, y_tr, X_val, y_val, label_prefix, filename_prefix):\n    # fit a fresh lgb model with early stopping to capture evals_result\n    import lightgbm as lgb\n    params = clf.get_params().copy()\n    num_boost_round = params.get(\"n_estimators\", 100)\n    lgb_train = lgb.Dataset(X_tr, label=y_tr)\n    lgb_val = lgb.Dataset(X_val, label=y_val, reference=lgb_train)\n    # adapt objective\n    if len(np.unique(y_tr)) > 2:\n        params.setdefault(\"objective\", \"multiclass\")\n        params[\"num_class\"] = len(np.unique(y_tr))\n        metric = \"multi_logloss\"\n    else:\n        params.setdefault(\"objective\", \"binary\")\n        metric = \"binary_logloss\"\n    params[\"verbose\"] = -1\n    evals_result = {}\n    bst = lgb.train(params, lgb_train, num_boost_round=num_boost_round,\n                    valid_sets=[lgb_train, lgb_val],\n                    valid_names=[\"train\",\"validation\"],\n                    early_stopping_rounds=20,\n                    evals_result=evals_result,\n                    verbose_eval=False)\n    # extract\n    metric_name = list(evals_result[\"train\"].keys())[0]\n    train_vals = evals_result[\"train\"][metric_name]\n    val_vals = evals_result[\"validation\"][metric_name]\n    epochs = list(range(1, len(train_vals)+1))\n\n    fig, ax = plt.subplots()\n    ax.plot(epochs, train_vals, label=f\"train {metric_name}\")\n    ax.plot(epochs, val_vals, label=f\"val {metric_name}\")\n    ax.axvline(bst.best_iteration, color='grey', linestyle='--', label=f\"best_iteration={bst.best_iteration}\")\n    ax.set_xlabel(\"Boosting round\")\n    ax.set_ylabel(metric_name)\n    ax.set_title(f\"{label_prefix} {metric_name} per boosting round\")\n    ax.legend()\n    plt.tight_layout()\n    fig.savefig(f\"{filename_prefix}_{metric_name}.png\")\n    plt.close(fig)\n    return bst.best_iteration\n\n# 1) Confusion matrices for stacking (train & test)\nprint(\"Plotting confusion matrices for stacking classifier (train & test)...\")\nlabels = stacking_clf.classes_\nplot_confusion(y_train_balanced, y_train_pred, labels=labels, title=\"Confusion Matrix - Train (Stacking)\",\n               filename=\"eval_plots/confusion_train_stacking.png\")\nplot_confusion(y_test, y_test_pred, labels=labels, title=\"Confusion Matrix - Test (Stacking)\",\n               filename=\"eval_plots/confusion_test_stacking.png\")\nprint(\"Saved confusion matrices to eval_plots/\")\n\n# 2) Retrain best_xgb and best_lgbm (on X_train_rfe) with a small validation split to record curves\nprint(\"Training XGBoost and LightGBM again with validation split to capture training curves and best_iteration...\")\nX_tr_sub, X_val_sub, y_tr_sub, y_val_sub = train_test_split(X_train_rfe, y_train_balanced, test_size=0.2, random_state=42, stratify=y_train_balanced)\n\nbest_iter_xgb = None\ntry:\n    if isinstance(best_xgb, XGBClassifier):\n        print(\"Retraining best_xgb to extract evals_result...\")\n        best_iter_xgb = plot_train_val_curves_from_xgb(best_xgb, X_tr_sub, y_tr_sub, X_val_sub, y_val_sub,\n                                                       label_prefix=\"XGBoost (best_xgb)\",\n                                                       filename_prefix=\"eval_plots/xgb\")\n        print(\"XGBoost best_iteration:\", best_iter_xgb)\nexcept Exception as e:\n    print(\"XGBoost curve plotting failed:\", e)\n\nbest_iter_lgbm = None\ntry:\n    import lightgbm as lgb\n    if isinstance(best_lgbm, LGBMClassifier):\n        print(\"Retraining best_lgbm to extract evals_result...\")\n        best_iter_lgbm = plot_train_val_curves_from_lgbm(best_lgbm, X_tr_sub, y_tr_sub, X_val_sub, y_val_sub,\n                                                         label_prefix=\"LightGBM (best_lgbm)\",\n                                                         filename_prefix=\"eval_plots/lgbm\")\n        print(\"LightGBM best_iteration:\", best_iter_lgbm)\nexcept Exception as e:\n    print(\"LightGBM curve plotting failed:\", e)\n\n# 3) If final estimator of stacking is XGB, retrain final estimator to inspect its curves as well\ntry:\n    final_est = stacking_clf.final_estimator_\n    if isinstance(final_est, XGBClassifier):\n        print(\"Retraining stacking final estimator (XGB) on same X_train_rfe to capture its curves...\")\n        # For stacking final, train on whole X_train_rfe (could also use sub-split)\n        fi_best_iter = plot_train_val_curves_from_xgb(final_est, X_tr_sub, y_tr_sub, X_val_sub, y_val_sub,\n                                                      label_prefix=\"Stacking final XGB\",\n                                                      filename_prefix=\"eval_plots/stacking_final_xgb\")\n        print(\"Stacking final XGB best_iteration:\", fi_best_iter)\nexcept Exception as e:\n    print(\"Stacking final estimator retrain failed or is not XGB:\", e)\n\n# 4) ROC and PR plots already computed earlier, but let's create nice displays for stacking predictions\nprint(\"Plotting ROC and Precision-Recall for stacking predictions...\")\ny_pred_proba = stacking_clf.predict_proba(X_test_rfe)\nn_classes = y_pred_proba.shape[1]\n\nif n_classes == 2:\n    # binary\n    positive_class = stacking_clf.classes_[1]\n    y_true_bin = (y_test == positive_class).astype(int)\n    RocCurveDisplay.from_predictions(y_true_bin, y_pred_proba[:,1])\n    plt.title(\"ROC - Stacking (binary)\")\n    plt.savefig(\"eval_plots/roc_stacking_binary.png\")\n    plt.close()\n\n    PrecisionRecallDisplay.from_predictions(y_true_bin, y_pred_proba[:,1])\n    plt.title(\"Precision-Recall - Stacking (binary)\")\n    plt.savefig(\"eval_plots/pr_stacking_binary.png\")\n    plt.close()\nelse:\n    # multiclass: plot per-class ROC and PR\n    y_test_bin = label_binarize(y_test, classes=stacking_clf.classes_)\n    # ROC per class\n    fig = plt.figure(figsize=(8,6))\n    for i, cls in enumerate(stacking_clf.classes_):\n        RocCurveDisplay.from_predictions(y_test_bin[:, i], y_pred_proba[:, i], name=f\"Class {cls}\")\n    plt.title(\"ROC per class - Stacking (multiclass)\")\n    plt.legend()\n    plt.savefig(\"eval_plots/roc_stacking_multiclass.png\")\n    plt.close()\n\n    # PR per class\n    fig = plt.figure(figsize=(8,6))\n    for i, cls in enumerate(stacking_clf.classes_):\n        PrecisionRecallDisplay.from_predictions(y_test_bin[:, i], y_pred_proba[:, i], name=f\"Class {cls}\")\n    plt.title(\"Precision-Recall per class - Stacking (multiclass)\")\n    plt.legend()\n    plt.savefig(\"eval_plots/pr_stacking_multiclass.png\")\n    plt.close()\n\nprint(\"Saved ROC/PR plots to eval_plots/\")\n\n# 5) Print and save classification reports\nfrom sklearn.metrics import classification_report\ntrain_report = classification_report(y_train_balanced, y_train_pred, digits=4)\ntest_report = classification_report(y_test, y_test_pred, digits=4)\nprint(\"Train Classification Report:\\n\", train_report)\nprint(\"Test Classification Report:\\n\", test_report)\nwith open(\"eval_plots/classification_reports.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(\"Train Classification Report\\n\")\n    f.write(train_report + \"\\n\\n\")\n    f.write(\"Test Classification Report\\n\")\n    f.write(test_report + \"\\n\\n\")\n\nprint(\"All additional evaluation artifacts saved in ./eval_plots/\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Show & save plots (confusion, ROC/PR, train/val curves, boxplots)\n\nimport os, sys\nimport matplotlib.pyplot as plt\nfrom IPython import get_ipython\nfrom sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay, PrecisionRecallDisplay\n\n# helper: show only if running inside an interactive env (not blocking scripts)\ndef maybe_show():\n    try:\n        if get_ipython() is not None:\n            plt.show()\n        else:\n            # likely script / headless: skip showing to avoid blocking\n            pass\n    except Exception:\n        pass\n\noutput_dir = \"eval_plots\"\nos.makedirs(output_dir, exist_ok=True)\n\n# 1) Confusion matrices (train & test) — save + show\nlabels = stacking_clf.classes_\n\n# train confusion\ncm_train_fig, ax = plt.subplots(figsize=(6,6))\ndisp_train = ConfusionMatrixDisplay.from_predictions(y_train_balanced, y_train_pred, display_labels=labels, ax=ax, cmap=plt.cm.Blues)\nax.set_title(\"Confusion Matrix - Train (Stacking)\")\nplt.tight_layout()\ntrain_cm_path = os.path.join(output_dir, \"confusion_train_stacking.png\")\ncm_train_fig.savefig(train_cm_path)\nmaybe_show()\nplt.close(cm_train_fig)\n\n# test confusion\ncm_test_fig, ax = plt.subplots(figsize=(6,6))\ndisp_test = ConfusionMatrixDisplay.from_predictions(y_test, y_test_pred, display_labels=labels, ax=ax, cmap=plt.cm.Blues)\nax.set_title(\"Confusion Matrix - Test (Stacking)\")\nplt.tight_layout()\ntest_cm_path = os.path.join(output_dir, \"confusion_test_stacking.png\")\ncm_test_fig.savefig(test_cm_path)\nmaybe_show()\nplt.close(cm_test_fig)\n\nprint(f\"Saved confusion matrices to {output_dir}/\")\n\n# 2) ROC & Precision-Recall (save + show)\ny_pred_proba = stacking_clf.predict_proba(X_test_rfe)\nn_classes = y_pred_proba.shape[1]\n\nif n_classes == 2:\n    positive_class = stacking_clf.classes_[1]\n    y_true_bin = (y_test == positive_class).astype(int)\n\n    roc_fig = plt.figure()\n    RocCurveDisplay.from_predictions(y_true_bin, y_pred_proba[:,1])\n    plt.title(\"ROC - Stacking (binary)\")\n    roc_path = os.path.join(output_dir, \"roc_stacking_binary.png\")\n    plt.tight_layout()\n    plt.savefig(roc_path)\n    maybe_show()\n    plt.close()\n\n    pr_fig = plt.figure()\n    PrecisionRecallDisplay.from_predictions(y_true_bin, y_pred_proba[:,1])\n    plt.title(\"Precision-Recall - Stacking (binary)\")\n    pr_path = os.path.join(output_dir, \"pr_stacking_binary.png\")\n    plt.tight_layout()\n    plt.savefig(pr_path)\n    maybe_show()\n    plt.close()\nelse:\n    y_test_bin = label_binarize(y_test, classes=stacking_clf.classes_)\n\n    # ROC per class\n    roc_fig = plt.figure(figsize=(8,6))\n    for i, cls in enumerate(stacking_clf.classes_):\n        RocCurveDisplay.from_predictions(y_test_bin[:, i], y_pred_proba[:, i], name=f\"Class {cls}\")\n    plt.title(\"ROC per class - Stacking (multiclass)\")\n    plt.legend()\n    roc_path = os.path.join(output_dir, \"roc_stacking_multiclass.png\")\n    plt.tight_layout()\n    plt.savefig(roc_path)\n    maybe_show()\n    plt.close()\n\n    # PR per class\n    pr_fig = plt.figure(figsize=(8,6))\n    for i, cls in enumerate(stacking_clf.classes_):\n        PrecisionRecallDisplay.from_predictions(y_test_bin[:, i], y_pred_proba[:, i], name=f\"Class {cls}\")\n    plt.title(\"Precision-Recall per class - Stacking (multiclass)\")\n    plt.legend()\n    pr_path = os.path.join(output_dir, \"pr_stacking_multiclass.png\")\n    plt.tight_layout()\n    plt.savefig(pr_path)\n    maybe_show()\n    plt.close()\n\nprint(f\"Saved ROC/PR plots to {output_dir}/\")\n\n# 3) Train/validation curves for XGBoost\n# If the earlier retrain functions returned best iterations saved as variables, we can re-run them here.\n# In case those functions weren't called earlier (or failed), try best-effort to retrain briefly to get curves.\n\nfrom sklearn.model_selection import train_test_split\nX_tr_sub, X_val_sub, y_tr_sub, y_val_sub = train_test_split(X_train_rfe, y_train_balanced, test_size=0.2, random_state=42, stratify=y_train_balanced)\n\ndef safe_plot_xgb_curves(clf, X_tr, y_tr, X_val, y_val, fname_prefix):\n    try:\n        import xgboost as xgb\n        params = clf.get_xgb_params()\n        num_round = clf.get_params().get(\"n_estimators\", 100)\n        dtrain = xgb.DMatrix(X_tr, label=y_tr)\n        dval = xgb.DMatrix(X_val, label=y_val)\n        if len(np.unique(y_tr)) > 2:\n            params.setdefault(\"objective\", \"multi:softprob\")\n            params[\"num_class\"] = len(np.unique(y_tr))\n        else:\n            params.setdefault(\"objective\", \"binary:logistic\")\n        params[\"verbosity\"] = 0\n        evals_result = {}\n        bst = xgb.train(params, dtrain, num_boost_round=num_round,\n                        evals=[(dtrain, \"train\"), (dval, \"validation\")],\n                        early_stopping_rounds=20,\n                        evals_result=evals_result,\n                        verbose_eval=False)\n        # plot metric\n        metric = list(evals_result[\"train\"].keys())[0]\n        train_vals = evals_result[\"train\"][metric]\n        val_vals = evals_result[\"validation\"][metric]\n        epochs = range(1, len(train_vals)+1)\n        fig, ax = plt.subplots()\n        ax.plot(epochs, train_vals, label=f\"train {metric}\")\n        ax.plot(epochs, val_vals, label=f\"val {metric}\")\n        if hasattr(bst, \"best_iteration\"):\n            ax.axvline(bst.best_iteration+1, color='grey', linestyle='--', label=f\"best_iter={bst.best_iteration+1}\")\n        ax.set_xlabel(\"Boosting round\")\n        ax.set_ylabel(metric)\n        ax.set_title(\"XGBoost train/val\")\n        ax.legend()\n        path = os.path.join(output_dir, f\"{fname_prefix}_xgb_{metric}.png\")\n        fig.savefig(path)\n        maybe_show()\n        plt.close(fig)\n        print(\"Saved XGBoost curve:\", path)\n    except Exception as e:\n        print(\"XGBoost curve plotting skipped/failed:\", e)\n\ndef safe_plot_lgbm_curves(clf, X_tr, y_tr, X_val, y_val, fname_prefix):\n    try:\n        import lightgbm as lgb\n        params = clf.get_params().copy()\n        num_round = params.get(\"n_estimators\", 100)\n        ltrain = lgb.Dataset(X_tr, label=y_tr)\n        lval = lgb.Dataset(X_val, label=y_val, reference=ltrain)\n        if len(np.unique(y_tr)) > 2:\n            params.setdefault(\"objective\", \"multiclass\")\n            params[\"num_class\"] = len(np.unique(y_tr))\n        else:\n            params.setdefault(\"objective\", \"binary\")\n        params[\"verbose\"] = -1\n        evals_result = {}\n        bst = lgb.train(params, ltrain, num_boost_round=num_round,\n                        valid_sets=[ltrain, lval],\n                        valid_names=[\"train\",\"validation\"],\n                        early_stopping_rounds=20,\n                        evals_result=evals_result,\n                        verbose_eval=False)\n        metric = list(evals_result[\"train\"].keys())[0]\n        train_vals = evals_result[\"train\"][metric]\n        val_vals = evals_result[\"validation\"][metric]\n        epochs = range(1, len(train_vals)+1)\n        fig, ax = plt.subplots()\n        ax.plot(epochs, train_vals, label=f\"train {metric}\")\n        ax.plot(epochs, val_vals, label=f\"val {metric}\")\n        ax.axvline(bst.best_iteration, color='grey', linestyle='--', label=f\"best_iter={bst.best_iteration}\")\n        ax.set_xlabel(\"Boosting round\")\n        ax.set_ylabel(metric)\n        ax.set_title(\"LightGBM train/val\")\n        ax.legend()\n        path = os.path.join(output_dir, f\"{fname_prefix}_lgbm_{metric}.png\")\n        fig.savefig(path)\n        maybe_show()\n        plt.close(fig)\n        print(\"Saved LightGBM curve:\", path)\n    except Exception as e:\n        print(\"LightGBM curve plotting skipped/failed:\", e)\n\n# call safe plotting on best models if available\ntry:\n    if 'best_xgb' in globals():\n        safe_plot_xgb_curves(best_xgb, X_tr_sub, y_tr_sub, X_val_sub, y_val_sub, \"best_xgb\")\n    if 'best_lgbm' in globals():\n        safe_plot_lgbm_curves(best_lgbm, X_tr_sub, y_tr_sub, X_val_sub, y_val_sub, \"best_lgbm\")\n    # if final estimator is xgb, try it as well\n    if isinstance(stacking_clf.final_estimator_, XGBClassifier):\n        safe_plot_xgb_curves(stacking_clf.final_estimator_, X_tr_sub, y_tr_sub, X_val_sub, y_val_sub, \"stacking_final_xgb\")\nexcept Exception as e:\n    print(\"Error while plotting train/val curves:\", e)\n\n# 4) Boxplots for final selected features (grouped by class) — save + show\ntry:\n    if len(final_features) > 0:\n        df_features = pd.DataFrame(X_train_rfe, columns=final_features)\n        df_features['label'] = y_train_balanced\n        # per-feature boxplot grouped by label\n        for feat in final_features:\n            fig = plt.figure(figsize=(6,4))\n            # pandas boxplot grouped by label\n            box = df_features.boxplot(column=feat, by='label')\n            plt.title(f\"{feat} by class\")\n            plt.suptitle(\"\")  # remove automatic suptitle\n            plt.xlabel(\"Class\")\n            plt.ylabel(feat)\n            path = os.path.join(output_dir, f\"boxplot_{feat}.png\")\n            fig = plt.gcf()\n            fig.savefig(path)\n            maybe_show()\n            plt.close()\n        print(\"Saved boxplots for final features to\", output_dir)\n    else:\n        print(\"No final_features available for boxplots.\")\nexcept Exception as e:\n    print(\"Boxplot generation failed:\", e)\n\n# 5) Save classification reports\nfrom sklearn.metrics import classification_report\ntrain_report = classification_report(y_train_balanced, y_train_pred, digits=4)\ntest_report = classification_report(y_test, y_test_pred, digits=4)\nwith open(os.path.join(output_dir, \"classification_reports.txt\"), \"w\", encoding=\"utf-8\") as f:\n    f.write(\"Train Classification Report\\n\")\n    f.write(train_report + \"\\n\\n\")\n    f.write(\"Test Classification Report\\n\")\n    f.write(test_report + \"\\n\\n\")\n\nprint(\"All additional evaluation artifacts saved in ./\" + output_dir + \"/\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}